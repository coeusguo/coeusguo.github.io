
<!DOCTYPE HTML>
<html>

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-EHLYGK132J"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-EHLYGK132J');
    </script>

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@100;300;400&display=swap" rel="stylesheet">

	<title>PCL</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- CSS only -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-+0n0xVW2eSR5OomGNYDnhzAbDsOXxcvSN1TPprVMTNDbiYZCxYbOOl7+AMvyTG2x" crossorigin="anonymous">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <link href="style.css" type="text/css" rel="stylesheet" media="screen,projection"/>
</head>

<body>
<br><br><br><br>
<div class="container">
        <div class="row text-center" style="font-size:38px">
            <div class="col">
                Learning Transferable Part-Level Representation by Language Supervision
            </div>
        </div>

        <br>
        <div class="row text-center" style="font-size:28px">
            <div class="col">
            CVPR 2022 Submission Under Review
            </div>
        </div>
        <br>

        <div class="h-100 row text-center heavy justify-content-md-center" style="font-size:24px;">
            <div class="col-sm-3">
                <a href="https://hkchengrex.github.io/">Shaopeng Guo</a>
            </div>
            <div class="col-sm-3">
                Yonglu Li
            </div>
            <div class="col-sm-3">
                Xinpeng Liu
            </div>
            <div class="col-sm-3">
                Xinyu Xu
            </div>
            <div class="col-sm-3">
                Cewu Lu
            </div>
            <div class="col-sm-3">
                Yu-Wing Tai
            </div>
            <div class="col-sm-3">
                Chi-Keung Tang
            </div>
        </div>

        <br>

        <div class="h-100 row text-center justify-content-md-center" style="font-size:20px;">
            <div class="col-sm-2">
                [Paper]
            </div>
            <div class="col-sm-2">
                [Code]
            </div>
        </div>

    <br>


    <hr>

    <div class="row" style="font-size:32px">
        <div class="col">
        Abstract
        </div>
    </div>
    <div class="row">
        <div class="col">
            <p style="text-align: justify;">
                Human-Object Interaction (HOI) detection plays an important role in human activity understanding. 
                Most previous methods solely rely on human instance-level supervision and thus cannot effectively disentangle the fine-grained action semantics (e.g., holding while eating). 
                Recently, <a href="https://arxiv.org/pdf/2004.00945.pdf" >PaStaNet</a> has proposed to address the problem by explicitly labeling the states of each body part with corresponding human pose and boxes in the images. 
                However, its network structure is very complicated and hard to train and make inferences due to the complex input.
                In this paper, we propose a novel framework that adopts a transformer-based detector that, besides using HOI labels, only employ human-part state annotations (i.e., without human part box position and human pose).
                Instead of directly applying discrete human-part labels during classification, we incorporate a language model BERT to transform natural language labels to continuous latent feature vectors and train our model by contrastive loss. 
                Moreover, we can further utilize our trained BERT to train our model with the weakly-labeled dataset without part state labels (e.g., AVA) by language supervision. 
                The experimental results on the HICO-DET dataset demonstrate the superiority of our method in terms of overall mAP <strong>(34.0 vs 29.1)</strong> and especially mAP of rare classes <strong>(31.9 vs 22.6)</strong> compared with the state-of-the-art methods.
            </p>
        </div>
    </div>

    <br>
    <hr>
    <div class="row" style="font-size:32px">
        <div class="col">
        Network Architecture
        </div>
    </div>
    <br>
    <div class="h-100 row text-center justify-content-md-center">
        <div class="col">
            <img width="100%" src="images/pcl_arch.png" alt="framework">
        </div>
    </div>

    <br>
    The figure gives an overview of our network architecture, which can be divided into three parts: 
    (1) a visual encoder takes images as input and outputs a set of embedded tokens;
    (2) the Instance Decoder (ID) decodes human and object bounding boxes with object class;
    (3) the Part-Level Decoder (PLD) takes the output of instance decoder as the memory input, further decodes PaSta and predicts verbs based on part-level information.
    Inspired by CLIP, instead of explicitly annotating the PaSta labels by discrete vectors, we adopt the pre-trained BERT obtained from CLIP to implicitly encode them as continuous feature vectors. 
    <br>
    <hr>

    <div class="row" style="font-size:32px">
        <div class="col">
        Training Pipeline
        </div>
    </div>
    <br>
    <div class="h-100 row text-center justify-content-md-center">
        <div class="col">
            <img width="65%" src="images/pcl_teaser.png" alt="framework">
        </div>
    </div>
    <br>
    The training pipeline can be divided into two phases: part-Level pretraining, and semi-supervised learning.
    Notably, we disable the HOI class layer and only use part-level supervision, which will be trained in fine-tuning process.

    <br>
    <hr>
    <div class="row" style="font-size:32px">
        <div class="col">
        Main Experiment Results on HICO-DET
        </div>
    </div>

    <br>

    <div class="h-100 row text-center justify-content-md-center">
        <div class="col">
            <table class="metric_table">
                <tr>
                    <th class='left_align' rowspan="2" scope="rowgroup">Method</th>
                    <th colspan="3" scope="colgroup" >Default mAP</th>
                    <th colspan="3" scope="colgroup" >Known Object mAP</th>
                </tr>
                <tr>
                    <th>overall</th>
                    <th>rare</th>
                    <th>non-rare</th>
                    <th>overall</th>
                    <th>rare</th>
                    <th>non-rare</th>
                </tr>
                <tr>
                    <td class='left_align'>TIN</td>
                    <td>17.0</td>
                    <td>13.4</td>
                    <td>18.1</td>
                    <td>19.2</td>
                    <td>15.5</td>
                    <td>20.3</td>
                </tr>
                <tr>
                    <td class='left_align'>DRG</td>
                    <td>24.5</td>
                    <td>19.5</td>
                    <td>26.0</td>
                    <td>28.0</td>
                    <td>23.1</td>
                    <td>29.4</td>
                </tr>
                <tr>
                    <td class='left_align'>HOTR</td>
                    <td>25.7</td>
                    <td>21.9</td>
                    <td>26.9</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td class='left_align'>IDN</td>
                    <td>26.3</td>
                    <td>22.6</td>
                    <td>27.4</td>
                    <td>28.2</td>
                    <td>24.5</td>
                    <td>29.4</td>
                </tr>
                <tr>
                    <td class='left_align'>QPIC</td>
                    <td>29.1</td>
                    <td>21.9</td>
                    <td>31.2</td>
                    <td>31.7</td>
                    <td>24.1</td>
                    <td>33.9</td>
                </tr>
                <tr>
                    <td class='left_align'>TIN-PaStaNet</td>
                    <td>22.7</td>
                    <td>21.2</td>
                    <td>23.1</td>
                    <td>24.5</td>
                    <td>23.0</td>
                    <td>25.0</td>
                </tr>
                <tr style="background-color:#99ff99">
                    <td class='left_align'>QPIC-PCL*</td>
                    <td>30.3</td>
                    <td>25.3</td>
                    <td>31.8</td>
                    <td>31.9</td>
                    <td>27.6</td>
                    <td>34.2</td>
                </tr>
                <tr style="background-color:#99ff99">
                    <td class='left_align'>QPIC-PCL*-AVA</td>
                    <td>30.4</td>
                    <td>25.2</td>
                    <td>32.0</td>
                    <td>32.1</td>
                    <td>27.6</td>
                    <td>34.5</td>
                </tr>
                <tr>
                    <td class='left_align'>PaStaNet*</td>
                    <td>19.5</td>
                    <td>17.3</td>
                    <td>20.1</td>
                    <td>22.9</td>
                    <td>20.5</td>
                    <td>22.5</td>
                </tr>
                <tr style="background-color:#99ff99">
                    <td class='left_align'>PCL*</td>
                    <td>32.1</td>
                    <td>27.8</td>
                    <td>32.8</td>
                    <td>34.9</td>
                    <td>32.4</td>
                    <td>35.7</td>
                </tr>
                <tr style="background-color:#99ff99">
                    <td class='left_align'>PCL</td>
                    <td>33.7</td>
                    <td>31.7</td>
                    <td>33.9</td>
                    <td>36.4</td>
                    <td>34.9</td>
                    <td>36.7</td>
                </tr>
                <tr style="background-color:#99ff99">
                    <td class='left_align'>PCL-AVA</td>
                    <td><strong>34.0</strong></td>
                    <td><strong>31.9</strong></td>
                    <td><strong>34.2</strong></td>
                    <td><strong>36.7</strong></td>
                    <td><strong>35.1</strong></td>
                    <td><strong>36.9</strong></td>
                </tr>
            </table>
            <br>

        </div>
    </div>

    <br>
    <div class="row">
        <div class="col">
            <p style="text-align: justify;">
                In the table, we show the results of various baselines and PCL with its variants, whose rows are marked in green color. 
                The last three rows are our proposed method with PaSta supervision, in which PCL* means the model is directly trained on HICO-DET with PaSta annotations. 
                The model shows significant improvement in the rare HOI class from 22.6 (IDN) to 27.8 with 5.2 mAP, while the overall mAP improved from 29.1 (QPIC) to 32.1 with 3.0 mAP. 
                With the pre-training on HAKE with PaSta labels, denoted by PCL, the overall mAP got further improved by 1.6 mAP from 32.1 to 33.7. 
                Moreover, the mAP in rare HOI classes improved by a large margin by 3.9 mAP from 27.8 to 31.7. 
                Lastly, we performed a semi-supervised training on AVA after pre-training on HAKE. 
                The mAP improves by a reasonable amount in all three metrics and pushes the boundary of the overall mAP to 34.0.
                Besides, we build a baseline based on QPIC, namely QPIC-PCL*, such that the model can be trained without PaSta annotations. 
                From the table, we can find that our method can still outperform QPIC by 1.2 mAP. 
                With semi-supervised training, denoted by QPIC-PCL*-AVA, the performance got minor improvement with 0.1 overall mAP. 
            </p>
        </div>
    </div>

    <br>
    <hr>
    <br>


<!-- 
    <div class="row" style="font-size:32px">
        <div class="col">
        Qualitative Results
        </div>
    </div>
    <br> -->
    <!-- <center>
        <iframe style="width:100%; aspect-ratio: 1.78;"
            src="https://www.youtube.com/embed/j88gG-foerw" 
            title="YouTube video player" frameborder="0" 
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
            allowfullscreen>
        </iframe>
    </center> -->


</div>

</body>
</html>
